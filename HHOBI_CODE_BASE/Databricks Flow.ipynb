{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f140457-399f-473c-9e17-e528f5c0eed7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a18e1cd-dd61-4664-ae55-fd6ff14775d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24c801bd-8357-450a-96f9-ae2032469271",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Beginning of the program\n",
    "load_control_table_entries = fetch_data_flows(spark).collect()\n",
    "\n",
    "for entry in load_control_table_entries:\n",
    "    tables = intialize_tables(spark, entry)\n",
    "    \n",
    "    if entry['sourceFormat'] == \"Oracle DB\":\n",
    "        url, jdbc_driver_value, username, password = get_jdbc_credentials(entry)\n",
    "        for table in tables:\n",
    "            print(\"Started table \" + table['targetTablename'], datetime.now())\n",
    "            if table_dependency_check(spark, table['loadDependency']):\n",
    "                if table['tableProperties']['tablesize'] == \"small\":\n",
    "                    load_data(spark, table, url, jdbc_driver_value, username, password)\n",
    "                \n",
    "                elif table['tableProperties']['tablesize'] == \"huge\":\n",
    "                    if table['splitLoad'] == \"N\":\n",
    "                        load_data(spark, table, url, jdbc_driver_value, username, password)\n",
    "                    elif table['splitLoad'] == \"Y\":\n",
    "                        recurrence = table['splitLoadConfigurations']['recurrence']  #monthly, quarterly, yearly\n",
    "                        cdc_column = table['splitLoadConfigurations']['splitColumn']\n",
    "                        split_load(spark, table, cdc_column, url, jdbc_driver_value, username, password, recurrence)\n",
    "                update_ingestion_entities_status(spark, \"Completed\", None, table['JobId'])\n",
    "            else:\n",
    "                print(\"Dependent tables are not completed\")\n",
    "    update_dataflow_status(spark, entry['dataFlowGroup'], \"Ready for DLT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5972adb5-c8c3-4f0d-b62a-abbfaeeaa0ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %run ./Functions/Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5bda617-56c5-474a-998e-3fbf3234844a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #Beginning of the program\n",
    "# load_control_table_entries = fetch_data_flows().collect()\n",
    "# # load_control_table_entries = get_load_control_table_entries().collect()\n",
    "\n",
    "# for entry in load_control_table_entries:\n",
    "#     tables = intialize_tables(entry)\n",
    "    \n",
    "#     if entry['sourceFormat'] == \"Oracle DB\":\n",
    "#         url, jdbc_driver_value, username, password = get_jdbc_credentials(entry)\n",
    "#         for table in tables:\n",
    "#             print(\"Started table \" + table['targetTablename'], datetime.now())\n",
    "#             if table_dependency_check(table['loadDependency']):\n",
    "#                 if table['tableProperties']['tablesize'] == \"small\":\n",
    "#                     load_data(table, url, jdbc_driver_value, username, password)\n",
    "                \n",
    "#                 elif table['tableProperties']['tablesize'] == \"huge\":\n",
    "#                     if table['splitLoad'] == \"N\":\n",
    "#                         load_data(table, url, jdbc_driver_value, username, password)\n",
    "#                     elif table['splitLoad'] == \"Y\":\n",
    "#                         recurrence = table['splitLoadConfigurations']['recurrence']  #monthly, quarterly, yearly\n",
    "#                         cdc_column = table['splitLoadConfigurations']['splitColumn']\n",
    "#                         split_load(table, cdc_column, url, jdbc_driver_value, username, password, recurrence)\n",
    "#                 update_ingestion_entities_status(\"Completed\", None, table['JobId'])\n",
    "#             else:\n",
    "#                 print(\"Dependent tables are not completed\")\n",
    "#     update_dataflow_status(entry['dataFlowGroup'], \"Ready for DLT\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 803427649455595,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks Flow",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
