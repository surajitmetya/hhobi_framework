{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e867293-b224-4daf-973f-55226c43238c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import date,timedelta,datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "\n",
    "audit_schema = \"prod_hhobi.hhobi_audit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "409c45c4-7be4-403e-93f0-2069d9a3471f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#encryption\n",
    "import base64\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "def column_native_encrypt(df_enc, Tokenizationdetails):\n",
    "    try:\n",
    "        Tokenizationdetails=Tokenizationdetails.split(\",\")\n",
    "        for column in Tokenizationdetails:\n",
    "            if (len(Tokenizationdetails)==1 and Tokenizationdetails[0]==\"NA\") or len(Tokenizationdetails)==0:\n",
    "                return df_enc\n",
    "            elif column in df_enc.columns:\n",
    "                df_enc = df_enc.withColumn(f\"{column}_encrypted\", expr(f\"main.tokenizer.Encrypt({column})\")).drop(column)\n",
    "                #print('here')\n",
    "                df_enc=df_enc.withColumnRenamed(f\"{column}_encrypted\",column)\n",
    "            else:\n",
    "                raise Exception()\n",
    "    except Exception as e:\n",
    "        print('invalid entry, Exception: ', e)\n",
    "    return df_enc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9232c2a1-17a3-4f47-bcd1-d1952dfa4b78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#VOLUME_TO_DLT\n",
    "import os\n",
    "import gnupg\n",
    "import zipfile\n",
    "import json\n",
    "from shutil import copyfile\n",
    "import time\n",
    "# Define paths\n",
    "\n",
    "\n",
    "\n",
    "def setup_gpg(*, secret_key: str):\n",
    "    gpg = gnupg.GPG()\n",
    "    import_result = gpg.import_keys(secret_key)\n",
    "    return gpg, import_result\n",
    "\n",
    "def copy_files_target(source_dir: str, target_dir: str, pattern: str,DEC_DIR_FLAG :str,UNZIP_DIR_FLAG : str):\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "    files = dbutils.fs.ls(source_dir)\n",
    "    \n",
    "    if DEC_DIR_FLAG=='N' and UNZIP_DIR_FLAG=='Y':\n",
    "        files_to_copy = [f.path for f in files if f.name.endswith('.zip') and pattern in f.name]\n",
    "        for file_path in files_to_copy:\n",
    "            file_name = file_path.split('/')[-1]\n",
    "            target_path = f\"{target_dir}/{file_name}\"\n",
    "            dbutils.fs.cp(file_path, target_path)\n",
    "    elif DEC_DIR_FLAG=='Y' and UNZIP_DIR_FLAG=='N':\n",
    "        files_to_copy = [f.path for f in files if f.name.endswith('.dat') and pattern in f.name]\n",
    "        for file_path in files_to_copy:\n",
    "            file_name = file_path.split('/')[-1]\n",
    "            target_path = f\"{target_dir}/{file_name}\"\n",
    "            dbutils.fs.cp(file_path, target_path)\n",
    "    else:\n",
    "        files_to_copy = [f.path for f in files if f.name.endswith('.dat') and pattern in f.name]\n",
    "        for file_path in files_to_copy:\n",
    "            file_name = file_path.split('/')[-1]\n",
    "            target_path = f\"{target_dir}/{file_name}\"\n",
    "            dbutils.fs.cp(file_path, target_path)\n",
    "        \n",
    "\n",
    "def decrypt_files(*, gpg: gnupg.GPG, passphrase: str,source_dir:str, dec_dir: str,pattern: str):\n",
    "    #copy_files_to_dec_dir(source_dir=source_path, target_dir=dec_dir, pattern=pattern)\n",
    "    if not os.path.exists(dec_dir):\n",
    "        os.makedirs(dec_dir)\n",
    "\n",
    "    list_of_files = os.listdir(source_dir)\n",
    "    for file in list_of_files:\n",
    "         if pattern in file and file.endswith('.zip.gpg') :\n",
    "            encrypted_file = os.path.join(source_dir, file)\n",
    "            decrypted_file = os.path.join(dec_dir, file[:-4])\n",
    "\n",
    "            # Decrypt the file\n",
    "            with open(encrypted_file, 'rb') as encrypted_f:\n",
    "                status = gpg.decrypt_file(encrypted_f, passphrase=passphrase, always_trust=True, output=decrypted_file)\n",
    "                \n",
    "\n",
    "def unzip_files(unzip_dir,dec_dir_src, pattern):\n",
    "    if not os.path.exists(unzip_dir):\n",
    "        os.makedirs(unzip_dir)\n",
    "    list_of_files = os.listdir(dec_dir_src)\n",
    "    zip_files = [file for file in list_of_files if file.endswith('.zip')]\n",
    "    for file in zip_files:\n",
    "        if pattern in file:\n",
    "            zip_file_with_pattern = os.path.join(dec_dir_src, file)\n",
    "            # print(file.split('/')[-1])\n",
    "            with zipfile.ZipFile(zip_file_with_pattern, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(unzip_dir)\n",
    "    \n",
    "\n",
    "def Volumes_to_DLT(target_table_name, source_path,dec_dir, unzip_dir, pattern, file_format, DEC_DIR_FLAG, UNZIP_DIR_FLAG,encrypted_col,quality = 'bronze'):\n",
    "    \n",
    "    @dlt.table(\n",
    "        name=target_table_name,\n",
    "        comment=f\"Load {target_table_name} from volumes\"\n",
    "    )\n",
    "    def data_load():\n",
    "        # Get secrets\n",
    "        os.chdir(source_path)\n",
    "        scope_gpg = dbutils.secrets.get(scope=\"dataops\", key=\"gpg\")\n",
    "        spark.conf.set(\"credentials\", scope_gpg)\n",
    "        scope_split = json.loads(spark.conf.get(\"credentials\"))\n",
    "        passphrase = scope_split['passphrase']\n",
    "        secret_key = scope_split['secret_key'] \n",
    "        \n",
    "        # Setup GPG\n",
    "        gpg, import_result = setup_gpg(secret_key=secret_key)\n",
    "        \n",
    "        # Handle decryption and/or unzip based on flags\n",
    "        if DEC_DIR_FLAG == 'Y':\n",
    "            decrypt_files(gpg=gpg, passphrase=passphrase, source_dir=source_path, dec_dir=dec_dir, pattern=pattern)\n",
    "        else:\n",
    "            \n",
    "            copy_files_target(source_dir=source_path, target_dir=dec_dir, pattern=pattern,DEC_DIR_FLAG=DEC_DIR_FLAG,UNZIP_DIR_FLAG=UNZIP_DIR_FLAG)\n",
    "\n",
    "        if UNZIP_DIR_FLAG == 'Y':\n",
    "            time.sleep(20)\n",
    "            unzip_files(unzip_dir,dec_dir,pattern)\n",
    "        else:\n",
    "            copy_files_target(source_dir=dec_dir, target_dir=unzip_dir, pattern=pattern,DEC_DIR_FLAG=DEC_DIR_FLAG,UNZIP_DIR_FLAG=UNZIP_DIR_FLAG)\n",
    "        \n",
    "        # Load the data into a DataFrame\n",
    "    \n",
    "        file_to_load = unzip_dir\n",
    "        source_df=spark.readStream.format(\"cloudFiles\") \\\n",
    "            .option(\"cloudFiles.format\", file_format) \\\n",
    "            .option(\"delimiter\", \"\\u001c\") \\\n",
    "            .load(file_to_load)\n",
    "        source_df=column_native_encrypt(source_df, encrypted_col)\n",
    "        return source_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93002d19-5eb2-43da-82ad-3aae6ab9aa92",
     "showTitle": true,
     "title": "fetch_matching_rows_Load_Control"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_matching_rows_Load_Control(spark, param1=None, param2=None, param3=None):\n",
    "\n",
    "  query = f\"SELECT * FROM {audit_schema}.ingestion_load_control_table WHERE 1=1\" \n",
    "  if param1 is not None:\n",
    "    query += f\" AND dataGroupId = '{param1}'\" \n",
    "  if param2 is not None:\n",
    "    query += f\" AND dataFlowGroup = '{param2}'\"\n",
    "  if param3 is not None:\n",
    "    query += f\" AND dataFlowId = '{param3}'\"\n",
    "\n",
    "  matching_rows = spark.sql(query)\n",
    "  return matching_rows\n",
    "\n",
    "def fetch_matching_rows_Entities_List(spark, PK_Column):\n",
    "    query = f\"SELECT * FROM {audit_schema}.ingestion_entities WHERE dataFlowFK ='{PK_Column}'\"\n",
    "    print(query)\n",
    "    matching_records = spark.sql(query)\n",
    "\n",
    "    return matching_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15602dc4-32ad-4568-a755-8c5553199ff5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def Delta_to_bronze(target_Table, source_table_name, quality = 'bronze'):\n",
    "    print('Delta_to_bronze')\n",
    "\n",
    "    @dlt.table(\n",
    "        name=target_Table\n",
    "    )\n",
    "    def process_delta_table():\n",
    "        source_delta_table_path = f\"{source_table_name}\"\n",
    "        return (\n",
    "            spark.read.table(f\"{source_delta_table_path}\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08a05a09-2905-47b3-9ca3-0a83b51b6b67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #Volume to DLT\n",
    "# def Volumes_to_DLT(target_table_name, source_path, file_format):\n",
    "    \n",
    "#     @dlt.table(\n",
    "#         name=target_table_name,\n",
    "#         comment=f\"Load {target_table_name} from volumes\"\n",
    "#     )\n",
    "#     def data_load():\n",
    "#         df = spark.readStream.format(\"cloudFiles\") \\\n",
    "#             .option(\"cloudFiles.format\", file_format) \\\n",
    "#             .option(\"delimiter\", \"\\x1d\") \\\n",
    "#             .load(source_path)\n",
    "        \n",
    "#         return df\n",
    "\n",
    "#     return data_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e5a412f-739e-442c-bf01-10c6409f2e05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fetch_bronze_data_flows():\n",
    "    # batch_time = (datetime.now() + timedelta(minutes = 330)).hour\n",
    "    \n",
    "    query = f\"select * from {audit_schema}.ingestion_load_control_table where targetLayer = 'Bronze' and DataFlow_Status = 'Ready for DLT'\"  #.format(str(batch_time))\n",
    "    matching_records = spark.sql(query)\n",
    "\n",
    "    return matching_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7cfd6f9-c705-43e7-9630-1738e08c92ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def update_bronze_dataflow_status(dataFlowGroup, status):\n",
    "    query = f\"update {audit_schema}.ingestion_load_control_table set DataFlow_status = {status} where dataFlowGroup = {dataFlowGroup} and targetLayer in ('Bronze')\"\n",
    "    spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f5bd421-16bb-4eb9-afc0-d8e59a4e89f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"flow\").getOrCreate()\n",
    "\n",
    "# to be passed while running the DLT pipeline\n",
    "# dataGroupId = 'A'\n",
    "# dataFlowGroup = 'A1'\n",
    "# dataFlowId = 'A102'\n",
    "# TargetLayer = 'Bronze'\n",
    "\n",
    "data_flows = fetch_bronze_data_flows().collect()\n",
    "\n",
    "for data_flow in data_flows:\n",
    "    dataGroupId = data_flow['dataGroupId']\n",
    "    dataFlowGroup = data_flow['dataFlowGroup']\n",
    "    dataFlowId = data_flow['dataFlowId']\n",
    "    pk_column_value = dataGroupId + \"-\" + dataFlowGroup + \"-\" + dataFlowId\n",
    "    sourceFormat = data_flow['sourceFormat']\n",
    "    sourceDetails = data_flow['sourceDetails']\n",
    "    # quality = data_flow['Target_Layer']\n",
    "\n",
    "    # if data_flow.filter(\"sourceFormat = 'Oracle DB'\").count() > 0:\n",
    "    if sourceFormat == 'Oracle DB':\n",
    "        matching_records_table2 = fetch_matching_rows_Entities_List(spark, pk_column_value)\n",
    "        display(matching_records_table2)\n",
    "        for table_load in matching_records_table2.collect():\n",
    "            target_Table = table_load['targetTable']\n",
    "            primaryKey = table_load['primaryKey']  \n",
    "            source_table_name= table_load[('SourceTableName')] \n",
    "            delta_to_DLT(target_Table, source_table_name, quality = 'bronze')\n",
    "    \n",
    "    # elif data_flow.filter(\"sourceFormat = 'Delta Table'\").count() > 0:\n",
    "    elif sourceFormat == 'Delta Table':\n",
    "        matching_records_table2 = fetch_matching_rows_Entities_List(spark, pk_column_value)\n",
    "        #display(matching_records_table2)\n",
    "        for table_load in matching_records_table2.collect():\n",
    "            target_Table = table_load['targetTablename']\n",
    "            primaryKey = table_load['primaryKey']  \n",
    "            source_table_name= table_load[('SourceTableName')] \n",
    "            Delta_to_bronze(target_Table, source_table_name, quality = 'bronze')\n",
    "\n",
    "\n",
    "    # elif data_flow.filter(\"sourceFormat = 'Volumes'\").count() > 0:\n",
    "    elif sourceFormat == 'Files':\n",
    "        matching_records_table2 = fetch_matching_rows_Entities_List(spark, pk_column_value)\n",
    "        for table_load in matching_records_table2.collect():\n",
    "            target_tableName = table_load['targetTablename']\n",
    "            working_dir=table_load['volume_details']['working_dir']\n",
    "            dec_dir=table_load['volume_details']['dec_dir']\n",
    "            unzip_dir=table_load['volume_details']['unzip_dir']\n",
    "            file_pattern=table_load['volume_details']['file_pattern']\n",
    "            file_format=table_load['volume_details']['file_format']\n",
    "            is_encrypted=table_load['volume_details']['is_encrypted']\n",
    "            is_zipped=table_load['volume_details']['is_zipped']\n",
    "            encrypted_col=table_load['encrypted_col']\n",
    "            Volumes_to_DLT(target_tableName,working_dir,dec_dir,unzip_dir,file_pattern,file_format,is_encrypted,is_zipped,encrypted_col, quality = 'bronze')\n",
    "    else:\n",
    "            pass\n",
    "    #update_bronze_dataflow_status(data_flow['dataFlowGroup'], \"Completed\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2586236333148238,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Delta_To_DLT_bronze",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
